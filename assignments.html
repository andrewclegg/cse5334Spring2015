<!DOCTYPE html>
<html lang="en">
    <head>
        <link href="css/metro-bootstrap.min.css" rel="stylesheet">
        <link href="css/metro-bootstrap-responsive.min.css" rel="stylesheet">
        <link href="css/iconFont.min.css" rel="stylesheet">
        <link href="css/courses.css" rel="stylesheet">

        <script src="js/jquery.min.js"></script>
        <script src="js/jquery.widget.min.js"></script>
        <script src="js/metro.min.js"></script>

        <title>CSE 5334:002 Data Mining - Programming Assignments</title>

    </head>

    <body class="metro">

            <div class="navigation-bar">
                <div class="navigation-bar-content">
                    <a href="index.html" class="element"><strong>CSE 5334:002 Data Mining</strong></a>
                    <span class="element-divider"></span>

                    <a href="index.html" class="element"><strong>Home</strong></a>
                    <span class="element-divider"></span>
                    <a href="syllabus.html" class="element"><strong>Syllabus</strong></a>
                    <span class="element-divider"></span>
                    <a href="schedule.html" class="element"><strong>Schedule</strong></a>
                    <span class="element-divider"></span>
                    <a href="resources.html" class="element"><strong>Resources</strong></a>
                    <span class="element-divider"></span>
                    <a href="assignments.html" class="element"><strong>Assignments</strong></a>
                    <span class="element-divider"></span>
                    <a href="policies.html" class="element"><strong>Policies</strong></a>
                    <span class="element-divider"></span>

                    <div class="element input-element  place-right">
                        <form role="search" method="GET" action="https://www.google.com/search" target="_blank">
                            <div class="input-control text size6">
                                <input type="text" placeholder="Search Within Course Website"  name="as_q" >
                                <input type="hidden" name="as_sitesearch" value="saravanan-thirumuruganathan.github.io">
                                <button class="btn-search"></button>
                            </div>
                        </form>
                    </div>

                </div>
            </div>







        <div class="container">

            <h3 class="text-center">CSE 5334: Data Mining</h3>
                <h4 class="text-center">Section 002, Spring 2015</h4>
                <h4 class="text-center">TuTh 2:00PM - 3:20PM, PKH 321</h4>


                <div class="panel">
                    <div class="panel-header bg-lightBlue fg-white">
                        Programming Assignments
                    </div>
                    <div class="panel-content">

                        <ol>
                            <li> Exploratory Data Analysis using Python, Pandas, Matplotlib and Seaborn.</li>
                            <li> Classification and Regression Algorithms using Scikit-learn</li>
                            <li> Clustering using Scikit-learn</li>
                            <li> Search Engine Basics</li>
                            <li> Recommender Systems</li>
                            <li> Capstone Project: Putting it all together</li>
                        </ol>
        
                    <p>

                    <strong>Late days: </strong>
                    Each student has <b>5</b> late days to use at his or her discretion for the problem sets and programming assignments. One cannot use more than <b>2</b> days for one assignment or problem set without prior approval from the instructors. Please reserve your late days for legitimate emergencies. Each late day constitutes a 24-hour extension; you cannot split late days into smaller increments. If two partners on a programming exercise want to take a late day, they must contribute two days from their allotment; either one from each of them, or, with permission, two days from the person with extra late days.   
                    </p>

<p>
                    <strong>Late penalties:</strong>
Once a student runs out of late days, any late submissions are penalized at a rate of <b>50%</b> per day. No assignment may be handed in more than <b>2</b> days late.
</p>


                    </div>

                </div>


<p></p>
                <div class="panel">
                    <div class="panel-header bg-lightBlue fg-white">
                        PA I: Exploratory Analysis of FEC's 2012 Presidential Election Contribution Dataset 
                    </div>
                    <div class="panel-content">
                        <p>
                        In this assignment, you will be doing three different things. 
                        First, you will scrape content from three popular websites (Wikipedia, Walmart and Facebook).
                        Next, you will use Pandas to perform a guided exploration over FEC data.
                        Finally, you will use some basic 1-D and 2-D plots to visualize the data.
                        </p>

                        <p>
                            This is a LONG assignment. So please form a team and start early!
                        </p>

                        <p>
                            <b>Deadline: </b>  26-Feb, 2015 <br/>
                            <b>Dataset: </b> 
                                <a href="http://saravanan-thirumuruganathan.github.io/cse5334Spring2015/assignments/PA1/fec_2012_contribution_subset.csv.zip">
                                        [Link] (Zip file)</a> <br/>
                            <b>Dataset Description: </b> 
                                <a href="ftp://ftp.fec.gov/FEC/Presidential_Map/2012/DATA_DICTIONARIES/CONTRIBUTOR_FORMAT.txt">[Link]</a> <br/>
                            <b>Notebook: </b> 
                                    <a href="http://saravanan-thirumuruganathan.github.io/cse5334Spring2015/assignments/PA1/PA1_ExploratoryAnalysis_FEC_2012_PresidentialElections.ipynb">[IPynb]</a>
                                    <a href="http://nbviewer.ipython.org/url/saravanan-thirumuruganathan.github.io/cse5334Spring2015/assignments/PA1/PA1_ExploratoryAnalysis_FEC_2012_PresidentialElections.ipynb">[HTML]</a>
                                    <br/>

                            <b>Solution: </b>
                                <a href="http://saravanan-thirumuruganathan.github.io/cse5334Spring2015/assignments/PA1/soln_PA1_ExploratoryAnalysis_FEC_2012_PresidentialElections.ipynb">[IPynb]</a>
                                <a href="http://saravanan-thirumuruganathan.github.io/cse5334Spring2015/assignments/PA1/soln_PA1_ExploratoryAnalysis_FEC_2012_PresidentialElections.html">[HTML]</a>

                            <br/>

                            <b>Assignment Survey: </b>  
                                <a href="https://docs.google.com/forms/d/127vVj_R8oskvwwc0m6B61zAapGNkvaSVacAkKPmYIfs/viewform?usp=send_form">[Link]</a>
                                 <br/>
                                Note that this is an anonymous form. You will be required to login to ensure that each student submits only once.
                                But the responses are anonymous.

                        </p>

                    </div>
                </div>

<p></p>
                
                
                
                <div class="panel">
                    <div class="panel-header bg-lightBlue fg-white">
                        PA II: Classification of MNIST Handwritten Digits and Regression of House Prices through Boston Dataset
                    </div>
                    <div class="panel-content">
                        <p>
At a high level, you will be building and evaluating different classifiers for recognizing handwritten digits of MNIST dataset and also build and evaluate various regression models for predicting house prices in Boston. At a more granular level, you will be doing the following:
</p><p> 

<strong>1. Binary Classification of MNIST Dataset </strong> 
<br/>

In the first set of tasks, you will evaluate a number of popular classifiers for the task of recognizing handwritten digits from MNIST dataset. Specifically, we will focus on distinguishing between 7 and 9 which are known to be a hard pair. We will not use any sophisticated ideas from Computer Vision/Image Processing and use classifiers directly over the data. The idea is to show that lot of times, you can simply run a set of classifiers and still get great results. While I will be giving some basic classifier code, you will have some opportunity to improve them by tuning the parameters.
<br/>
<strong>2. Multi-Class Classification of MNIST Dataset</strong>
<br/>

In the second set of tasks, we will do multi-class classification where the idea is to classify the image to one of the ten digits (0-9). We will start with some basic classifiers that are intrinsically multi-class. Then we will learn about how to convert binary classifiers to multi-class classifiers and how scikit-learn makes it very easy.
<br/>
<strong>3. Exploration of Different Evaluation Metrics</strong>
<br/>

In the first two set of tasks, we will narrowly focus on accuracy - what fraction of our predictions were correct. However, there are a number of popular evaluation metrics. You will learn how (and when) to use these evaluation metrics.
<br/>
<strong>4. Parameter Tuning through Grid Search/Cross Validation and Parallelization</strong>
<br/>

This is an advanced topic where you will learn how to tune your classifier and find optimal parameters. We will explore two powerful techniques of grid search and parameter search. This is a very compute intensive task - so you will also explore how to leverage parallelization capabilities of IPython kernel to get results sooner.
<br/>
<strong>5. Evaluation of Various Regression Models for Boston Houses Dataset</strong>
<br/>

In the final set of tasks, we will use regression to predict Boston house prices. We will explore both Ordinary Least Squares and also explore other regression variants of popular classifiers such as decision trees and SVM.
                        </p>

                        <p>
                            This is a relatively easy assignment. But some of the classifiers might take a lot of time to run.
                            So please form a team and start early!
                        </p>

                        <p>
                            <b>Deadline: </b>  15-Mar, 2015 <br/>
                            <b>Dataset: </b> 
                                <a href="http://mldata.org/repository/data/download/matlab/mnist-original/">
                                        [Link]</a> (download the dataset in Matlab format). 
                                        The notebook also has some helper functions that can automatically download the data for you.
                                <br/>
                            <b>Notebook: </b> 
                                    <a href="http://saravanan-thirumuruganathan.github.io/cse5334Spring2015/assignments/PA2/PA2_MNIST_Classification_Boston_Regression.ipynb">[IPynb]</a>
                                    <a href="http://saravanan-thirumuruganathan.github.io/cse5334Spring2015/assignments/PA2/PA2_MNIST_Classification_Boston_Regression.html">[HTML]</a>
                                    <br/>
                            <b>Assignment Survey: </b>  
                                <a href="https://docs.google.com/forms/d/127vVj_R8oskvwwc0m6B61zAapGNkvaSVacAkKPmYIfs/viewform?usp=send_form">[Link]</a>
                                 <br/>
                                Note that this is an anonymous form. You will be required to login to ensure that each student submits only once.
                                But the responses are anonymous.
                        </p>

                    </div>
                </div>

<p></p>


                <div class="panel">
                    <div class="panel-header bg-lightBlue fg-white">
                        PA III: Clustering and Dimensionality Reduction
                    </div>
                    <div class="panel-content">


<p>In this assignment, you will explore the fascinating concepts of clustering and dimensionality reduction. Since the aim is to get some insights into how and when to use clustering, we will mostly stick to small synthetic datasets. However, for fun, at the end of this assignment, you will write some code to collect your friends information from Facebook and use it for clustering. I got some very interesting results when I run them against my friends&#39; accounts. At a more granular level, you will be doing the following tasks:</p>
<strong>1. Evaluation of k-Means over Diverse Datasets</strong>
<p>In the first task, you will explore how k-Means perform on datasets with diverse structure. You will also explore different ways of customizing $k$-Means and learn about how to find good initialization and good value of k. </p>
<strong>2. Evaluation of Hierarchical Clustering over Diverse Datasets</strong>
<p>In this task, you will explore hierarchical clustering over different datasets. You will also evaluate different ways to merge clusters and good way to find the cut-off point for breaking the dendrogram.</p>
<strong>3. Comparison of Clustering Evaluation Metrics</strong>
<p>In the class, we mostly focused on SSE measure for evaluating how good a cluster is. There are many other statistical measures, and you will test them in this task.</p>
<strong>4. Clustering your Facebook Friends</strong>
<p>In this task, you will use the scraping knowledge you gained from Programming Assignment 1 to scrape details about your Facebook friends. You will then convert them to an appropriate vector format before clustering them.</p>
<strong>5. Dimensionality Reduction</strong>
<p>Dimensionality reduction is a key data pre-processing technique. You will perform PCA, a popular dimensionality reduction technique, over few images to get an intuition. Then you will apply it to MNIST data from Programming Assignment 2 to see how it performs.</p>


                        <p>
                            <b>Deadline: </b>  5-Apr, 2015 <br/>
                            <b>Notebook: </b> 
                                    <a href="http://saravanan-thirumuruganathan.github.io/cse5334Spring2015/assignments/PA3/PA3_Clustering_DimensionalityReduction.ipynb">[IPynb]</a>
                                    <a href="http://saravanan-thirumuruganathan.github.io/cse5334Spring2015/assignments/PA2/PA3_Clustering_DimensionalityReduction.html">[HTML]</a>
                                    <br/>
                        </p>


                    </div>
                </div>

<p></p>



                <div class="panel">
                    <div class="panel-header bg-lightBlue fg-white">
                        Search Engines: Crawling, Answering Keyword Queries, Ranking and Advertisement
                    </div>
                    <div class="panel-content">
                        <p>
                            Out : 05-Apr, Due: 26-Apr
                        </p>
                    </div>
                </div>

<p></p>



                <div class="panel">
                    <div class="panel-header bg-lightBlue fg-white">
                            Product (Movie?) Recommendation
                    </div>
                    <div class="panel-content">
                        <p>
                            Out : 26-Apr, Due: 10-May
                        </p>
                    </div>
                </div>

<p></p>



                <div class="panel">
                    <div class="panel-header bg-lightBlue fg-white">
                        Final Project
                    </div>
                    <div class="panel-content">
                        <p>
                            <b>Deadline: </b>  10-May, 2015 <br/>
                        </p>

                        <p>
The objective of this project is to show your mastery of data mining by performing some non-trivial, challenging and fun project
that will use one or more major concepts discussed in the class.
I will create a Piazza post with more suggestions and additional details.
At a high level, your proposed project has to fall into one of the three categories:
<ol>
    <li> Do some analysis on a known dataset (benchmark datasets, KDD Cup, Kaggle, data.gov or from state government of Texas etc). 
            
    <li> Learn and apply some interesting concept not discussed in class or explore in-depth some topic discussed in the class. 
            Examples for the former might include Topic Models, Social Network Analysis, Anomaly Detection, Mining in Computational Finance,
                Mining in Election polls, Bayesian Statistics, data analytics in NoSQL databases etc.
            Examples for the latter could include Deep Learning/Neural Networks, Sampling, Computational Advertising, SVM, Feature Selection, 
                Recommender Systems, Ensembles, challenges in High dimensional data (images, text, biological etc), 
                Model testing on Web etc.
    <li> Do a non-trivial mining project using some popular open source tool, package or resource.
            Examples might include Social Media APIs (Twitter, Facebook, Yelp, NYTimes),
                 Visualization Software (D3, Tableau software), 
                 Large Scale processing (MapReduce, Apache Spark, AWS/AzureML/Google Predict/Mahout),
                 Graph processing (GraphLab, Giraph, Pregel),
                 Deep Learning (Torch, Caffe), 
                 Online learning (Vowpal Wabbit).

</ol>
        </p>
        <p>
If you cannot choose a project, I would suggest the <strong>Click Prediction</strong> project.
In this project, you will predict the click through rate of ads given a query, the ads (link) information, and user information.
This was originally proposed in 2012 KDD Cup Track 2.
Additional details of the data and task can be found <a href="https://www.kddcup2012.org/c/kddcup2012-track2">here</a>.
The original dataset is divided into 3 parts: training, testing, and maps from feature id to features. 
The training set has 150M instances, and the testing data has 20M instances. 
Drs. Carlos Guestrin and Emily Fox from University of Washington have subsampled and simplified this dataset by 
joining the training and testing data with the feature maps. 
After spending couple of hours, I noticed that (linear) models that we have covered in class work quite well!
Doing this project has additional advantages too.
While we covered the basics of advertising in the class, you will be starting as an advertiser and focus a lot on tools and techniques than domain knowledge.
Also, this is a cool enough project to put in portfolio and getting some knowledge in this field wouldn't hurt.
There are lot of discussion in the internet and there is an entire journal issue for papers that describe what the top-performing teams did.
Data can be found <a href="http://courses.cs.washington.edu/courses/cse599c1/13wi/datasets/clickprediction_data.zip">here</a>.

                        </p>
                    </div>
                </div>

<p></p>


        </div>        

    </body>
</html>
